{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2076934a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard sinusoidal positional encoding.\n",
    "    Produces (batch, seq_len, d_model) given (batch, seq_len, d_model) input.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, max_len: int = 500):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)  # (max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)  # (max_len, 1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2, dtype=torch.float32) *\n",
    "            (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "\n",
    "class TrinucTransformerClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int = 64,\n",
    "        seq_len: int = 200,\n",
    "        d_model: int = 128,\n",
    "        n_heads: int = 4,\n",
    "        num_layers: int = 3,\n",
    "        dim_feedforward: int = 512,\n",
    "        dropout: float = 0.1,\n",
    "        num_classes: int = 2,  # 2 for binary logit (CrossEntropyLoss)\n",
    "        use_cls_token: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.d_model = d_model\n",
    "        self.use_cls_token = use_cls_token\n",
    "\n",
    "        # Token embedding (trinucleotides indexed 0..63)\n",
    "        self.token_emb = nn.Embedding(vocab_size + (1 if use_cls_token else 0), d_model)\n",
    "        # Positional encoding\n",
    "        self.pos_enc = PositionalEncoding(d_model, max_len=seq_len + (1 if use_cls_token else 0))\n",
    "\n",
    "        # Transformer encoder (batch_first=True => (B, L, E))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.dropout_head = nn.Dropout(dropout)\n",
    "\n",
    "        # Classification head: pooled representation -> logit(s)\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len) of integer token IDs in [0, vocab_size)\n",
    "        Returns:\n",
    "            logits: (batch_size, num_classes)\n",
    "        \"\"\"\n",
    "        # Embed tokens\n",
    "        x = self.token_emb(x)  # (B, L, d_model)\n",
    "\n",
    "        if self.use_cls_token:\n",
    "            batch_size = x.size(0)\n",
    "            cls_token = torch.zeros(batch_size, 1, self.d_model, device=x.device)\n",
    "            # Optional: learn a cls embedding instead of zeros\n",
    "            # self.cls_emb = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "            # cls_token = self.cls_emb.expand(batch_size, -1, -1)\n",
    "            x = torch.cat([cls_token, x], dim=1)  # (B, L+1, d_model)\n",
    "\n",
    "        # Add positional encoding\n",
    "        x = self.pos_enc(x)    # (B, L, d_model)\n",
    "\n",
    "        # Transformer encoder\n",
    "        x = self.encoder(x)    # (B, L, d_model)\n",
    "\n",
    "        if self.use_cls_token:\n",
    "            x = x[:, 0, :]       # (B, d_model), CLS\n",
    "        else:\n",
    "            x = x.mean(dim=1)    # (B, d_model)\n",
    "        \n",
    "        x = self.dropout_head(x)\n",
    "        logits = self.fc(x)    # (B, num_classes)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39a5b21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, Regex\n",
    "from itertools import product\n",
    "\n",
    "# 1. Generate the vocabulary for all 64 trinucleotides (AAA, AAC, ..., TTT)\n",
    "bases = ['A', 'C', 'G', 'T']\n",
    "trinucleotides = [''.join(p) for p in product(bases, repeat=3)]\n",
    "vocab = {tri: i for i, tri in enumerate(trinucleotides)}\n",
    "\n",
    "# 2. Add special tokens if your model uses them (e.g., [PAD], [CLS])\n",
    "# Your model uses index 64 for CLS if use_cls_token=True\n",
    "vocab[\"[CLS]\"] = 64\n",
    "\n",
    "# 3. Create a WordLevel Tokenizer\n",
    "tokenizer = Tokenizer(models.WordLevel(vocab=vocab, unk_token=\"[UNK]\"))\n",
    "\n",
    "# 4. Set a Pre-tokenizer to split DNA strings into 3-character chunks\n",
    "# This tells the tokenizer to look at \"ATGCAT\" as [\"ATG\", \"CAT\"]\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Split(\n",
    "    pattern=Regex(\".{3}\"), \n",
    "    behavior=\"isolated\"\n",
    ")\n",
    "\n",
    "# 5. Save the tokenizer files\n",
    "tokenizer.save(\"hf_trinuc_model/tokenizer.json\")\n",
    "\n",
    "tokenizer_config = {\n",
    "    \"model_max_length\": 200,\n",
    "    \"padding_side\": \"right\",\n",
    "    \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
    "    \"cls_token\": \"[CLS]\",\n",
    "    \"unk_token\": \"[UNK]\"\n",
    "}\n",
    "with open(\"hf_trinuc_model/tokenizer_config.json\", \"w\") as f:\n",
    "    json.dump(tokenizer_config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ae22dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PretrainedConfig\n",
    "\n",
    "class TrinucTransformerConfig(PretrainedConfig):\n",
    "    model_type = \"trinuc_transformer\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size=64,\n",
    "        seq_len=200,\n",
    "        d_model=256,\n",
    "        n_heads=4,\n",
    "        num_layers=4,\n",
    "        dim_feedforward=512,\n",
    "        dropout=0.1,\n",
    "        num_classes=2,\n",
    "        use_cls_token=True,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_len = seq_len\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.dropout = dropout\n",
    "        self.num_classes = num_classes\n",
    "        self.use_cls_token = use_cls_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a4ea3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import PreTrainedModel\n",
    "\n",
    "class TrinucTransformerModel(PreTrainedModel):\n",
    "    config_class = TrinucTransformerConfig\n",
    "    base_model_prefix = \"transformer\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        # Instantiate your original model logic here\n",
    "        self.model = TrinucTransformerClassifier(\n",
    "            vocab_size=config.vocab_size,\n",
    "            seq_len=config.seq_len,\n",
    "            d_model=config.d_model,\n",
    "            n_heads=config.n_heads,\n",
    "            num_layers=config.num_layers,\n",
    "            dim_feedforward=config.dim_feedforward,\n",
    "            dropout=config.dropout,\n",
    "            num_classes=config.num_classes,\n",
    "            use_cls_token=config.use_cls_token\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids=None, labels=None, **kwargs):\n",
    "        # input_ids: (batch_size, seq_len)\n",
    "        logits = self.model(input_ids)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss(label_smoothing=0.05)\n",
    "            loss = loss_fct(logits.view(-1, self.config.num_classes), labels.view(-1))\n",
    "\n",
    "        # Standard HF output format: (loss, logits) or just (logits,)\n",
    "        return {\"loss\": loss, \"logits\": logits} if loss is not None else {\"logits\": logits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ebf9f449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create config and model\n",
    "config = TrinucTransformerConfig()\n",
    "hf_model = TrinucTransformerModel(config)\n",
    "\n",
    "# 2. Load your raw .pth weights\n",
    "# Important: If your .pth was saved as the internal 'TrinucTransformerClassifier', \n",
    "# you might need to map keys to match 'self.model' prefix.\n",
    "signal = 'TIS'\n",
    "state_dict = torch.load(f\"./{signal}/Transformer_{signal}_all.pth\", map_location=\"cpu\")['model']\n",
    "hf_model.model.load_state_dict(state_dict)\n",
    "\n",
    "# 3. Save as HF compatible directory\n",
    "hf_model.save_pretrained(f\"../app/models/{signal}_model\")\n",
    "\n",
    "# must update config.json\n",
    "# \n",
    "#   \"architectures\": [\n",
    "#     \"DistilBertForSequenceClassification\"\n",
    "#   ],\n",
    "#   \"model_type\": \"distilbert\",\n",
    "#   \"id2label\": {\n",
    "#       \"0\": \"Negative\",\n",
    "#       \"1\": \"Positive\"\n",
    "#     },\n",
    "#     \"label2id\": {\n",
    "#       \"Negative\": 0,\n",
    "#       \"Positive\": 1\n",
    "#     },  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9284a34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randint(0, 64, (2, 198))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "17d2dd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install optimum onnx onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "85c21a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(\n",
    "    hf_model, \n",
    "    dummy_input, \n",
    "    f\"../app/models/{signal}_model/onnx/model.onnx\",\n",
    "    input_names=['input_ids'],     # Essential for JS compatibility\n",
    "    output_names=['logits'],       # Essential for JS compatibility\n",
    "    dynamic_axes={'input_ids': {0: 'batch_size', 1: 'sequence_length'}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "51b6b431",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "model_onnx = onnx.load(f\"../app/models/{signal}_model/onnx/model.onnx\")\n",
    "onnx.checker.check_model(model_onnx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
